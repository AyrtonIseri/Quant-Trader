{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from utils.nn_data_classifier import load_data, Classifier\n",
    "from utils.preprocess import preprocess, RNNDataset\n",
    "\n",
    "writer = SummaryWriter(log_dir='logs')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>Price</th>\n",
       "      <th>Volume_ETH</th>\n",
       "      <th>Price_BTC</th>\n",
       "      <th>Volume_BTC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>2016-03-14 10:00:00</td>\n",
       "      <td>13.100</td>\n",
       "      <td>0.008360</td>\n",
       "      <td>414.70</td>\n",
       "      <td>1.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>2016-03-14 14:00:00</td>\n",
       "      <td>14.000</td>\n",
       "      <td>248.973956</td>\n",
       "      <td>414.95</td>\n",
       "      <td>9.077592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>318</th>\n",
       "      <td>2016-03-14 19:00:00</td>\n",
       "      <td>14.750</td>\n",
       "      <td>8.442300</td>\n",
       "      <td>414.50</td>\n",
       "      <td>1.679111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>468</th>\n",
       "      <td>2016-03-15 00:00:00</td>\n",
       "      <td>12.650</td>\n",
       "      <td>651.351595</td>\n",
       "      <td>415.90</td>\n",
       "      <td>0.481963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>490</th>\n",
       "      <td>2016-03-15 01:00:00</td>\n",
       "      <td>12.576</td>\n",
       "      <td>4.558064</td>\n",
       "      <td>415.79</td>\n",
       "      <td>0.615654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2903842</th>\n",
       "      <td>2022-11-12 16:00:00</td>\n",
       "      <td>1274.800</td>\n",
       "      <td>9.022555</td>\n",
       "      <td>16934.00</td>\n",
       "      <td>0.065520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2903902</th>\n",
       "      <td>2022-11-12 17:00:00</td>\n",
       "      <td>1269.900</td>\n",
       "      <td>52.030265</td>\n",
       "      <td>16888.00</td>\n",
       "      <td>0.006411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2903962</th>\n",
       "      <td>2022-11-12 18:00:00</td>\n",
       "      <td>1272.700</td>\n",
       "      <td>0.252397</td>\n",
       "      <td>16899.00</td>\n",
       "      <td>0.006868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2904022</th>\n",
       "      <td>2022-11-12 19:00:00</td>\n",
       "      <td>1269.600</td>\n",
       "      <td>2.803365</td>\n",
       "      <td>16877.00</td>\n",
       "      <td>0.130167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2904082</th>\n",
       "      <td>2022-11-12 20:00:00</td>\n",
       "      <td>1269.000</td>\n",
       "      <td>0.305341</td>\n",
       "      <td>16905.00</td>\n",
       "      <td>0.022600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50208 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       time     Price  Volume_ETH  Price_BTC  Volume_BTC\n",
       "128     2016-03-14 10:00:00    13.100    0.008360     414.70    1.500000\n",
       "201     2016-03-14 14:00:00    14.000  248.973956     414.95    9.077592\n",
       "318     2016-03-14 19:00:00    14.750    8.442300     414.50    1.679111\n",
       "468     2016-03-15 00:00:00    12.650  651.351595     415.90    0.481963\n",
       "490     2016-03-15 01:00:00    12.576    4.558064     415.79    0.615654\n",
       "...                     ...       ...         ...        ...         ...\n",
       "2903842 2022-11-12 16:00:00  1274.800    9.022555   16934.00    0.065520\n",
       "2903902 2022-11-12 17:00:00  1269.900   52.030265   16888.00    0.006411\n",
       "2903962 2022-11-12 18:00:00  1272.700    0.252397   16899.00    0.006868\n",
       "2904022 2022-11-12 19:00:00  1269.600    2.803365   16877.00    0.130167\n",
       "2904082 2022-11-12 20:00:00  1269.000    0.305341   16905.00    0.022600\n",
       "\n",
       "[50208 rows x 5 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "historical_data = load_data()\n",
    "\n",
    "historical_data['minute'] = historical_data.time.dt.minute\n",
    "historical_data = historical_data[historical_data.minute == 0]\n",
    "historical_data.drop('minute', axis=1, inplace=True)\n",
    "\n",
    "historical_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classify data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>Price</th>\n",
       "      <th>Volume_ETH</th>\n",
       "      <th>Price_BTC</th>\n",
       "      <th>Volume_BTC</th>\n",
       "      <th>Classification</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>2016-03-14 10:00:00</td>\n",
       "      <td>13.100</td>\n",
       "      <td>0.008360</td>\n",
       "      <td>414.70</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>2016-03-14 14:00:00</td>\n",
       "      <td>14.000</td>\n",
       "      <td>248.973956</td>\n",
       "      <td>414.95</td>\n",
       "      <td>9.077592</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>318</th>\n",
       "      <td>2016-03-14 19:00:00</td>\n",
       "      <td>14.750</td>\n",
       "      <td>8.442300</td>\n",
       "      <td>414.50</td>\n",
       "      <td>1.679111</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>468</th>\n",
       "      <td>2016-03-15 00:00:00</td>\n",
       "      <td>12.650</td>\n",
       "      <td>651.351595</td>\n",
       "      <td>415.90</td>\n",
       "      <td>0.481963</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>490</th>\n",
       "      <td>2016-03-15 01:00:00</td>\n",
       "      <td>12.576</td>\n",
       "      <td>4.558064</td>\n",
       "      <td>415.79</td>\n",
       "      <td>0.615654</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2903722</th>\n",
       "      <td>2022-11-12 14:00:00</td>\n",
       "      <td>1265.100</td>\n",
       "      <td>1.905119</td>\n",
       "      <td>16851.00</td>\n",
       "      <td>0.161496</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2903782</th>\n",
       "      <td>2022-11-12 15:00:00</td>\n",
       "      <td>1276.600</td>\n",
       "      <td>21.640138</td>\n",
       "      <td>16923.00</td>\n",
       "      <td>0.381346</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2903842</th>\n",
       "      <td>2022-11-12 16:00:00</td>\n",
       "      <td>1274.800</td>\n",
       "      <td>9.022555</td>\n",
       "      <td>16934.00</td>\n",
       "      <td>0.065520</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2903902</th>\n",
       "      <td>2022-11-12 17:00:00</td>\n",
       "      <td>1269.900</td>\n",
       "      <td>52.030265</td>\n",
       "      <td>16888.00</td>\n",
       "      <td>0.006411</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2903962</th>\n",
       "      <td>2022-11-12 18:00:00</td>\n",
       "      <td>1272.700</td>\n",
       "      <td>0.252397</td>\n",
       "      <td>16899.00</td>\n",
       "      <td>0.006868</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50206 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       time     Price  Volume_ETH  Price_BTC  Volume_BTC  \\\n",
       "128     2016-03-14 10:00:00    13.100    0.008360     414.70    1.500000   \n",
       "201     2016-03-14 14:00:00    14.000  248.973956     414.95    9.077592   \n",
       "318     2016-03-14 19:00:00    14.750    8.442300     414.50    1.679111   \n",
       "468     2016-03-15 00:00:00    12.650  651.351595     415.90    0.481963   \n",
       "490     2016-03-15 01:00:00    12.576    4.558064     415.79    0.615654   \n",
       "...                     ...       ...         ...        ...         ...   \n",
       "2903722 2022-11-12 14:00:00  1265.100    1.905119   16851.00    0.161496   \n",
       "2903782 2022-11-12 15:00:00  1276.600   21.640138   16923.00    0.381346   \n",
       "2903842 2022-11-12 16:00:00  1274.800    9.022555   16934.00    0.065520   \n",
       "2903902 2022-11-12 17:00:00  1269.900   52.030265   16888.00    0.006411   \n",
       "2903962 2022-11-12 18:00:00  1272.700    0.252397   16899.00    0.006868   \n",
       "\n",
       "         Classification  \n",
       "128                   1  \n",
       "201                   0  \n",
       "318                   0  \n",
       "468                   1  \n",
       "490                   1  \n",
       "...                 ...  \n",
       "2903722               1  \n",
       "2903782               0  \n",
       "2903842               0  \n",
       "2903902               0  \n",
       "2903962               0  \n",
       "\n",
       "[50206 rows x 6 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_outlook = 2\n",
    "classi = Classifier(historical_data)\n",
    "classified_data = classi.classify_data_strict_time(time_outlook=time_outlook)\n",
    "classified_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sort data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>Price</th>\n",
       "      <th>Volume_ETH</th>\n",
       "      <th>Price_BTC</th>\n",
       "      <th>Volume_BTC</th>\n",
       "      <th>Classification</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016-03-14 10:00:00</td>\n",
       "      <td>13.100</td>\n",
       "      <td>0.008360</td>\n",
       "      <td>414.70</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2016-03-14 14:00:00</td>\n",
       "      <td>14.000</td>\n",
       "      <td>248.973956</td>\n",
       "      <td>414.95</td>\n",
       "      <td>9.077592</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2016-03-14 19:00:00</td>\n",
       "      <td>14.750</td>\n",
       "      <td>8.442300</td>\n",
       "      <td>414.50</td>\n",
       "      <td>1.679111</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2016-03-15 00:00:00</td>\n",
       "      <td>12.650</td>\n",
       "      <td>651.351595</td>\n",
       "      <td>415.90</td>\n",
       "      <td>0.481963</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2016-03-15 01:00:00</td>\n",
       "      <td>12.576</td>\n",
       "      <td>4.558064</td>\n",
       "      <td>415.79</td>\n",
       "      <td>0.615654</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50201</th>\n",
       "      <td>2022-11-12 14:00:00</td>\n",
       "      <td>1265.100</td>\n",
       "      <td>1.905119</td>\n",
       "      <td>16851.00</td>\n",
       "      <td>0.161496</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50202</th>\n",
       "      <td>2022-11-12 15:00:00</td>\n",
       "      <td>1276.600</td>\n",
       "      <td>21.640138</td>\n",
       "      <td>16923.00</td>\n",
       "      <td>0.381346</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50203</th>\n",
       "      <td>2022-11-12 16:00:00</td>\n",
       "      <td>1274.800</td>\n",
       "      <td>9.022555</td>\n",
       "      <td>16934.00</td>\n",
       "      <td>0.065520</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50204</th>\n",
       "      <td>2022-11-12 17:00:00</td>\n",
       "      <td>1269.900</td>\n",
       "      <td>52.030265</td>\n",
       "      <td>16888.00</td>\n",
       "      <td>0.006411</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50205</th>\n",
       "      <td>2022-11-12 18:00:00</td>\n",
       "      <td>1272.700</td>\n",
       "      <td>0.252397</td>\n",
       "      <td>16899.00</td>\n",
       "      <td>0.006868</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50206 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     time     Price  Volume_ETH  Price_BTC  Volume_BTC  \\\n",
       "0     2016-03-14 10:00:00    13.100    0.008360     414.70    1.500000   \n",
       "1     2016-03-14 14:00:00    14.000  248.973956     414.95    9.077592   \n",
       "2     2016-03-14 19:00:00    14.750    8.442300     414.50    1.679111   \n",
       "3     2016-03-15 00:00:00    12.650  651.351595     415.90    0.481963   \n",
       "4     2016-03-15 01:00:00    12.576    4.558064     415.79    0.615654   \n",
       "...                   ...       ...         ...        ...         ...   \n",
       "50201 2022-11-12 14:00:00  1265.100    1.905119   16851.00    0.161496   \n",
       "50202 2022-11-12 15:00:00  1276.600   21.640138   16923.00    0.381346   \n",
       "50203 2022-11-12 16:00:00  1274.800    9.022555   16934.00    0.065520   \n",
       "50204 2022-11-12 17:00:00  1269.900   52.030265   16888.00    0.006411   \n",
       "50205 2022-11-12 18:00:00  1272.700    0.252397   16899.00    0.006868   \n",
       "\n",
       "       Classification  \n",
       "0                   1  \n",
       "1                   0  \n",
       "2                   0  \n",
       "3                   1  \n",
       "4                   1  \n",
       "...               ...  \n",
       "50201               1  \n",
       "50202               0  \n",
       "50203               0  \n",
       "50204               0  \n",
       "50205               0  \n",
       "\n",
       "[50206 rows x 6 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = classified_data.sort_values('time').reset_index(drop=True)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQ_LEN = 24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = preprocess(sequence_length=SEQ_LEN)\n",
    "training, validation, testing  = processor.preprocess(dataframe=data, validation_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "class RNN_module(nn.Module):\n",
    "    def __init__(self, hidden_size, input_size, output_size, num_layers, prob):\n",
    "        super(RNN_module, self).__init__()\n",
    "        self._num_layers = num_layers\n",
    "        self._input_size = input_size\n",
    "        self._hidden_size = hidden_size\n",
    "        self._output_size = output_size\n",
    "\n",
    "        self.lstm1 = nn.LSTM(input_size = self._input_size, hidden_size = self._hidden_size, \n",
    "                            num_layers = self._num_layers, batch_first = True)\n",
    "        self._droupout = nn.Dropout(p = prob)\n",
    "        self._batch_norm = nn.BatchNorm1d(SEQ_LEN)\n",
    "\n",
    "        self._lstm2 = nn.LSTM(input_size = self._hidden_size, hidden_size = self._hidden_size, \n",
    "                            num_layers = self._num_layers, batch_first = True)\n",
    "\n",
    "        self.fc = nn.Linear(in_features=self._hidden_size, out_features= self._output_size)\n",
    "        self._relu = nn.ReLU()\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"RNN LSTM Model w/ {self._input_size} features and {self._num_layers} layers and {self._hidden_size} of hidden size\"\n",
    "\n",
    "    def forward(self, input):\n",
    "        \n",
    "        lstm1_output, (h_n, c_n) = self.lstm1(input)\n",
    "        droupout_output = self._droupout(lstm1_output)\n",
    "        out = self._batch_norm(droupout_output)\n",
    "\n",
    "        out, (h_n, c_n) = self._lstm2(out)\n",
    "        out = self._droupout(out)\n",
    "\n",
    "        pred = self.fc(out[:, -1, :])\n",
    "        pred = self._relu(pred)\n",
    "\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "learning_rate = 0.01\n",
    "dim_size = training._features.size(dim=-1)\n",
    "hidden_size = 100\n",
    "batch_size = 32\n",
    "number_of_classes = training[1][1].shape[0]\n",
    "layers = 1\n",
    "\n",
    "train_dataloader = DataLoader(training, batch_size = batch_size, shuffle = True)\n",
    "validation_dataloader = DataLoader(validation, batch_size = batch_size, shuffle = True)\n",
    "test_dataloader = DataLoader(testing, batch_size = batch_size, shuffle = False)\n",
    "\n",
    "model = RNN_module(hidden_size = hidden_size, input_size = dim_size,\n",
    "                     output_size = number_of_classes, num_layers = layers, prob=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.5090, 0.4910])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = training._labels\n",
    "classes, _ = torch.sort(torch.unique(labels, dim = 0))\n",
    "weights = torch.zeros((classes.shape[0],))\n",
    "\n",
    "total = len(data)\n",
    "\n",
    "for idx, _class in enumerate(classes):\n",
    "    freq = len(data[data.Classification == _class.item()])\n",
    "    weights[idx] += 1 - freq/total\n",
    "\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss(weight=weights)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECKPOINT_PATH = './models_parameters/LSTM/checkpoints_2/'\n",
    "BEST_PATH = './models_parameters/LSTM/best_model.pth'\n",
    "\n",
    "def epoch_training(model, train_dataloader, epoch, total_epochs, optimizer):\n",
    "    running_correct = 0\n",
    "    running_loss = 0.0\n",
    "    n_of_steps = len(train_dataloader)\n",
    "\n",
    "    for current_batch, (sequence, label) in enumerate(train_dataloader):\n",
    "        #forward: we are calculating the loss given the parameters\n",
    "        outputs = model(sequence)\n",
    "        loss = criterion(input=outputs, target = label.float())\n",
    "\n",
    "        #backward: lets update the parameters given the current loss\n",
    "        optimizer.zero_grad() #nullifies the current gradients. If you don't do this, gradients will be added up (you don't want that)\n",
    "        loss.backward() #computates the bwrd-prop gradient for each model parameter\n",
    "        optimizer.step() #updates the model current parameter using the gradients.\n",
    "\n",
    "        predictions = torch.argmax(outputs, 1)\n",
    "        correct = torch.argmax(label, 1)\n",
    "\n",
    "        running_correct += (predictions == correct).sum().item()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        if (current_batch + 1) % 50 == 0:\n",
    "            print(f\"epoch {epoch+1}/{total_epochs}, current step(batch): {current_batch+1}/{n_of_steps}, loss = {loss.item():.4f} \")\n",
    "            writer.add_scalar('training loss: ', running_loss/50, epoch * n_of_steps + current_batch)\n",
    "            writer.add_scalar('accuracy: ', running_correct/50, epoch * n_of_steps + current_batch)\n",
    "            running_loss = 0.0\n",
    "            running_correct = 0\n",
    "\n",
    "    writer.add_scalar('Epoch loss: ', loss, epoch + 1)\n",
    "\n",
    "\n",
    "def epoch_validate(model, validation_dataloader, epoch, total_epochs):\n",
    "    with torch.no_grad():\n",
    "        n_corrects = 0\n",
    "        n_samples = 0\n",
    "\n",
    "        for current_batch, (sequence, label) in enumerate(validation_dataloader):\n",
    "\n",
    "            #forward: we are calculating the loss given the parameters\n",
    "            outputs = model(sequence)\n",
    "            predictions = torch.argmax(outputs, 1)\n",
    "\n",
    "            n_samples += outputs.shape[0]\n",
    "            n_corrects += (predictions == label).sum().item()\n",
    "\n",
    "        acc = 100.0 * n_corrects / n_samples\n",
    "\n",
    "        print(f\"epoch {epoch+1}/{total_epochs} accuracy: {acc}\")\n",
    "        writer.add_scalar('Validation Accuracy: ', acc, epoch+1)\n",
    "\n",
    "    return acc\n",
    "\n",
    "\n",
    "def train_loop(model: RNN_module, train_dataloader: DataLoader, validation_dataloader: DataLoader, epochs: int, optimizer: torch.optim):\n",
    "    \n",
    "    max_accuracy = 0\n",
    "    is_best = False\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        epoch_training(model, train_dataloader, epoch, epochs, optimizer)\n",
    "\n",
    "        accuracy = epoch_validate(model, validation_dataloader, epoch, epochs)\n",
    "\n",
    "        if accuracy > max_accuracy:\n",
    "            is_best = True\n",
    "            max_accuracy = accuracy\n",
    "        else:\n",
    "            is_best = False\n",
    "        \n",
    "        checkpoint = {\n",
    "            'epoch': epoch+1,\n",
    "            'model_state': model.state_dict(),\n",
    "            'optim_state': optimizer.state_dict()\n",
    "        }\n",
    "\n",
    "        if is_best:\n",
    "            torch.save(checkpoint, BEST_PATH)\n",
    "        \n",
    "        torch.save(checkpoint, CHECKPOINT_PATH+f'model_{epoch+1}.pth')\n",
    "\n",
    "def overfit_batch(model: RNN_module, train_dataloader: DataLoader, epochs: int, optimizer: torch.optim):\n",
    "\n",
    "    running_correct = 0\n",
    "    running_loss = 0.0\n",
    "\n",
    "    sequence, label = next(iter(train_dataloader))\n",
    "    sequence2, label2 = next(iter(train_dataloader))\n",
    "\n",
    "    sequence = torch.cat([sequence, sequence2])\n",
    "    label = torch.cat([label, label2])\n",
    "\n",
    "    batch_size = len(label)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        #forward: we are calculating the loss given the parameters\n",
    "        outputs = model(sequence)\n",
    "        loss = criterion(input=outputs, target = label.float())\n",
    "\n",
    "        #backward: lets update the parameters given the current loss\n",
    "        optimizer.zero_grad() #nullifies the current gradients. If you don't do this, gradients will be added up (you don't want that)\n",
    "        loss.backward() #computates the bwrd-prop gradient for each model parameter\n",
    "        optimizer.step() #updates the model current parameter using the gradients.\n",
    "\n",
    "        predictions = torch.argmax(outputs, 1)\n",
    "        correct = torch.argmax(label, 1)\n",
    "\n",
    "        running_correct += (predictions == correct).sum().item() / batch_size\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        if (epoch + 1) % 1000 == 0:\n",
    "            print(f\"epoch {epoch+1}/{epochs}, loss = {running_loss/1000:.4f}, accuracy = {running_correct/1000*100:.4f}\")\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_correct = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_loop(test_dataloader: DataLoader, model: nn.Module):\n",
    "    with torch.no_grad():\n",
    "        n_corrects = 0\n",
    "        n_samples = 0\n",
    "\n",
    "        for current_batch, (sequence, label) in enumerate(test_dataloader):\n",
    "            #forward: we are calculating the loss given the parameters\n",
    "            outputs = model(sequence)\n",
    "            predictions = torch.argmax(outputs, 1)\n",
    "\n",
    "            n_samples += outputs.shape[0]\n",
    "            n_corrects += (predictions == label).sum().item()\n",
    "\n",
    "            if (current_batch + 1) % 200 == 0:\n",
    "                print(f\"test batch: {current_batch+1}/{len(test_dataloader)}, current accuracy: {100 * n_corrects / n_samples}\")\n",
    "\n",
    "        acc = 100.0 * n_corrects / n_samples\n",
    "        print(f\"final test accuracy: {acc}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1/10, current step(batch): 50/1098, loss = 0.3477 \n",
      "epoch 1/10, current step(batch): 100/1098, loss = 0.3481 \n",
      "epoch 1/10, current step(batch): 150/1098, loss = 0.3454 \n",
      "epoch 1/10, current step(batch): 200/1098, loss = 0.3470 \n",
      "epoch 1/10, current step(batch): 250/1098, loss = 0.3477 \n",
      "epoch 1/10, current step(batch): 300/1098, loss = 0.3462 \n",
      "epoch 1/10, current step(batch): 350/1098, loss = 0.3438 \n",
      "epoch 1/10, current step(batch): 400/1098, loss = 0.3450 \n",
      "epoch 1/10, current step(batch): 450/1098, loss = 0.3450 \n",
      "epoch 1/10, current step(batch): 500/1098, loss = 0.3466 \n",
      "epoch 1/10, current step(batch): 550/1098, loss = 0.3470 \n",
      "epoch 1/10, current step(batch): 600/1098, loss = 0.3474 \n",
      "epoch 1/10, current step(batch): 650/1098, loss = 0.3458 \n",
      "epoch 1/10, current step(batch): 700/1098, loss = 0.3481 \n",
      "epoch 1/10, current step(batch): 750/1098, loss = 0.3470 \n",
      "epoch 1/10, current step(batch): 800/1098, loss = 0.3474 \n",
      "epoch 1/10, current step(batch): 850/1098, loss = 0.3462 \n",
      "epoch 1/10, current step(batch): 900/1098, loss = 0.3466 \n",
      "epoch 1/10, current step(batch): 950/1098, loss = 0.3470 \n",
      "epoch 1/10, current step(batch): 1000/1098, loss = 0.3474 \n",
      "epoch 1/10, current step(batch): 1050/1098, loss = 0.3474 \n",
      "epoch 1/10 accuracy: 48.52047424529242\n",
      "epoch 2/10, current step(batch): 50/1098, loss = 0.3470 \n",
      "epoch 2/10, current step(batch): 100/1098, loss = 0.3477 \n",
      "epoch 2/10, current step(batch): 150/1098, loss = 0.3466 \n",
      "epoch 2/10, current step(batch): 200/1098, loss = 0.3470 \n",
      "epoch 2/10, current step(batch): 250/1098, loss = 0.3474 \n",
      "epoch 2/10, current step(batch): 300/1098, loss = 0.3477 \n",
      "epoch 2/10, current step(batch): 350/1098, loss = 0.3446 \n",
      "epoch 2/10, current step(batch): 400/1098, loss = 0.3470 \n",
      "epoch 2/10, current step(batch): 450/1098, loss = 0.3481 \n",
      "epoch 2/10, current step(batch): 500/1098, loss = 0.3458 \n",
      "epoch 2/10, current step(batch): 550/1098, loss = 0.3462 \n",
      "epoch 2/10, current step(batch): 600/1098, loss = 0.3481 \n",
      "epoch 2/10, current step(batch): 650/1098, loss = 0.3454 \n",
      "epoch 2/10, current step(batch): 700/1098, loss = 0.3489 \n",
      "epoch 2/10, current step(batch): 750/1098, loss = 0.3454 \n",
      "epoch 2/10, current step(batch): 800/1098, loss = 0.3466 \n",
      "epoch 2/10, current step(batch): 850/1098, loss = 0.3462 \n",
      "epoch 2/10, current step(batch): 900/1098, loss = 0.3462 \n",
      "epoch 2/10, current step(batch): 950/1098, loss = 0.3462 \n",
      "epoch 2/10, current step(batch): 1000/1098, loss = 0.3470 \n",
      "epoch 2/10, current step(batch): 1050/1098, loss = 0.3466 \n",
      "epoch 2/10 accuracy: 48.52047424529242\n",
      "epoch 3/10, current step(batch): 50/1098, loss = 0.3481 \n",
      "epoch 3/10, current step(batch): 100/1098, loss = 0.3485 \n",
      "epoch 3/10, current step(batch): 150/1098, loss = 0.3454 \n",
      "epoch 3/10, current step(batch): 200/1098, loss = 0.3462 \n",
      "epoch 3/10, current step(batch): 250/1098, loss = 0.3470 \n",
      "epoch 3/10, current step(batch): 300/1098, loss = 0.3454 \n",
      "epoch 3/10, current step(batch): 350/1098, loss = 0.3470 \n",
      "epoch 3/10, current step(batch): 400/1098, loss = 0.3446 \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [14], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m epochs \u001b[39m=\u001b[39m \u001b[39m10\u001b[39m\n\u001b[0;32m----> 2\u001b[0m train_loop(model, train_dataloader\u001b[39m=\u001b[39;49mtrain_dataloader, validation_dataloader \u001b[39m=\u001b[39;49m validation_dataloader, epochs\u001b[39m=\u001b[39;49mepochs, optimizer\u001b[39m=\u001b[39;49moptimizer)\n\u001b[1;32m      3\u001b[0m test_loop(test_dataloader\u001b[39m=\u001b[39mtest_dataloader, model\u001b[39m=\u001b[39mmodel)\n\u001b[1;32m      4\u001b[0m \u001b[39m# overfit_batch(model, train_dataloader=train_dataloader, epochs=epochs, optimizer=optimizer)\u001b[39;00m\n",
      "Cell \u001b[0;32mIn [11], line 63\u001b[0m, in \u001b[0;36mtrain_loop\u001b[0;34m(model, train_dataloader, validation_dataloader, epochs, optimizer)\u001b[0m\n\u001b[1;32m     60\u001b[0m is_best \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m     62\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(epochs):\n\u001b[0;32m---> 63\u001b[0m     epoch_training(model, train_dataloader, epoch, epochs, optimizer)\n\u001b[1;32m     65\u001b[0m     accuracy \u001b[39m=\u001b[39m epoch_validate(model, validation_dataloader, epoch, epochs)\n\u001b[1;32m     67\u001b[0m     \u001b[39mif\u001b[39;00m accuracy \u001b[39m>\u001b[39m max_accuracy:\n",
      "Cell \u001b[0;32mIn [11], line 16\u001b[0m, in \u001b[0;36mepoch_training\u001b[0;34m(model, train_dataloader, epoch, total_epochs, optimizer)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[39m#backward: lets update the parameters given the current loss\u001b[39;00m\n\u001b[1;32m     15\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad() \u001b[39m#nullifies the current gradients. If you don't do this, gradients will be added up (you don't want that)\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m loss\u001b[39m.\u001b[39;49mbackward() \u001b[39m#computates the bwrd-prop gradient for each model parameter\u001b[39;00m\n\u001b[1;32m     17\u001b[0m optimizer\u001b[39m.\u001b[39mstep() \u001b[39m#updates the model current parameter using the gradients.\u001b[39;00m\n\u001b[1;32m     19\u001b[0m predictions \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39margmax(outputs, \u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.4/envs/Quant-Trader/lib/python3.10/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    489\u001b[0m )\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.4/envs/Quant-Trader/lib/python3.10/site-packages/torch/autograd/__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    192\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    194\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    198\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    199\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "train_loop(model, train_dataloader=train_dataloader, validation_dataloader = validation_dataloader, epochs=epochs, optimizer=optimizer)\n",
    "test_loop(test_dataloader=test_dataloader, model=model)\n",
    "# overfit_batch(model, train_dataloader=train_dataloader, epochs=epochs, optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using best model in validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lstm1.weight_ih_l0 gradient: 0.00e+00\n",
      "lstm1.weight_hh_l0 gradient: 0.00e+00\n",
      "lstm1.bias_ih_l0 gradient: 0.00e+00\n",
      "lstm1.bias_hh_l0 gradient: 0.00e+00\n",
      "_batch_norm.weight gradient: 0.00e+00\n",
      "_batch_norm.bias gradient: 0.00e+00\n",
      "_lstm2.weight_ih_l0 gradient: 0.00e+00\n",
      "_lstm2.weight_hh_l0 gradient: 0.00e+00\n",
      "_lstm2.bias_ih_l0 gradient: 0.00e+00\n",
      "_lstm2.bias_hh_l0 gradient: 0.00e+00\n",
      "fc.weight gradient: 0.00e+00\n",
      "fc.bias gradient: 0.00e+00\n"
     ]
    }
   ],
   "source": [
    "for name, parameter in model.named_parameters():\n",
    "    print(f'{name} gradient: {parameter.grad.norm():.2e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = RNN_module(hidden_size = hidden_size, input_size = dim_size,\n",
    "                     output_size = number_of_classes, num_layers = 1)\n",
    "\n",
    "checkpoint = torch.load(BEST_PATH)\n",
    "print(f'Model type: {best_model}')\n",
    "print(f'Best performing model found at {checkpoint[\"epoch\"]}ºepoch')\n",
    "\n",
    "best_model.load_state_dict(state_dict=checkpoint['model_state'], strict=True)\n",
    "best_model.eval()\n",
    "\n",
    "test_loop(test_dataloader=test_dataloader, model=best_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load('./models_parameters/LSTM/checkpoints_2/model_1.pth')\n",
    "\n",
    "a = checkpoint['model_state']\n",
    "\n",
    "checkpoint = torch.load('./models_parameters/LSTM/checkpoints_2/model_10.pth')\n",
    "\n",
    "b = checkpoint['model_state']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit ('Quant-Trader')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fc0d1250504fb5812c35f9d788d06f80c95c7322289624d7d7f522231bf576a4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
