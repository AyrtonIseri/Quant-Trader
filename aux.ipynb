{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from utils.nn_data_classifier import load_data, Classifier\n",
    "from utils.preprocess import preprocess, RNNDataset\n",
    "writer = SummaryWriter(log_dir='logs')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interacao atual: 0/44584\n",
      "Interacao atual: 445/44584\n",
      "Interacao atual: 890/44584\n",
      "Interacao atual: 1335/44584\n",
      "Interacao atual: 1780/44584\n",
      "Interacao atual: 2225/44584\n",
      "Interacao atual: 2670/44584\n",
      "Interacao atual: 3115/44584\n",
      "Interacao atual: 3560/44584\n",
      "Interacao atual: 4005/44584\n",
      "Interacao atual: 4450/44584\n",
      "Interacao atual: 4895/44584\n",
      "Interacao atual: 5340/44584\n",
      "Interacao atual: 5785/44584\n",
      "Interacao atual: 6230/44584\n",
      "Interacao atual: 6675/44584\n",
      "Interacao atual: 7120/44584\n",
      "Interacao atual: 7565/44584\n",
      "Interacao atual: 8010/44584\n",
      "Interacao atual: 8455/44584\n",
      "Interacao atual: 8900/44584\n",
      "Interacao atual: 9345/44584\n",
      "Interacao atual: 9790/44584\n",
      "Interacao atual: 10235/44584\n",
      "Interacao atual: 10680/44584\n",
      "Interacao atual: 11125/44584\n",
      "Interacao atual: 11570/44584\n",
      "Interacao atual: 12015/44584\n",
      "Interacao atual: 12460/44584\n",
      "Interacao atual: 12905/44584\n",
      "Interacao atual: 13350/44584\n",
      "Interacao atual: 13795/44584\n",
      "Interacao atual: 14240/44584\n",
      "Interacao atual: 14685/44584\n",
      "Interacao atual: 15130/44584\n",
      "Interacao atual: 15575/44584\n",
      "Interacao atual: 16020/44584\n",
      "Interacao atual: 16465/44584\n",
      "Interacao atual: 16910/44584\n",
      "Interacao atual: 17355/44584\n",
      "Interacao atual: 17800/44584\n",
      "Interacao atual: 18245/44584\n",
      "Interacao atual: 18690/44584\n",
      "Interacao atual: 19135/44584\n",
      "Interacao atual: 19580/44584\n",
      "Interacao atual: 20025/44584\n",
      "Interacao atual: 20470/44584\n",
      "Interacao atual: 20915/44584\n",
      "Interacao atual: 21360/44584\n",
      "Interacao atual: 21805/44584\n",
      "Interacao atual: 22250/44584\n",
      "Interacao atual: 22695/44584\n",
      "Interacao atual: 23140/44584\n",
      "Interacao atual: 23585/44584\n",
      "Interacao atual: 24030/44584\n",
      "Interacao atual: 24475/44584\n",
      "Interacao atual: 24920/44584\n",
      "Interacao atual: 25365/44584\n",
      "Interacao atual: 25810/44584\n",
      "Interacao atual: 26255/44584\n",
      "Interacao atual: 26700/44584\n",
      "Interacao atual: 27145/44584\n",
      "Interacao atual: 27590/44584\n",
      "Interacao atual: 28035/44584\n",
      "Interacao atual: 28480/44584\n",
      "Interacao atual: 28925/44584\n",
      "Interacao atual: 29370/44584\n",
      "Interacao atual: 29815/44584\n",
      "Interacao atual: 30260/44584\n",
      "Interacao atual: 30705/44584\n",
      "Interacao atual: 31150/44584\n",
      "Interacao atual: 31595/44584\n",
      "Interacao atual: 32040/44584\n",
      "Interacao atual: 32485/44584\n",
      "Interacao atual: 32930/44584\n",
      "Interacao atual: 33375/44584\n",
      "Interacao atual: 33820/44584\n",
      "Interacao atual: 34265/44584\n",
      "Interacao atual: 34710/44584\n",
      "Interacao atual: 35155/44584\n",
      "Interacao atual: 35600/44584\n",
      "Interacao atual: 36045/44584\n",
      "Interacao atual: 36490/44584\n",
      "Interacao atual: 36935/44584\n",
      "Interacao atual: 37380/44584\n",
      "Interacao atual: 37825/44584\n",
      "Interacao atual: 38270/44584\n",
      "Interacao atual: 38715/44584\n",
      "Interacao atual: 39160/44584\n",
      "Interacao atual: 39605/44584\n",
      "Interacao atual: 40050/44584\n",
      "Interacao atual: 40495/44584\n",
      "Interacao atual: 40940/44584\n",
      "Interacao atual: 41385/44584\n",
      "Interacao atual: 41830/44584\n",
      "Interacao atual: 42275/44584\n",
      "Interacao atual: 42720/44584\n",
      "Interacao atual: 43165/44584\n",
      "Interacao atual: 43610/44584\n",
      "Interacao atual: 44055/44584\n",
      "Interacao atual: 44500/44584\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Day</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>Price</th>\n",
       "      <th>Volume BTC</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Classification</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>525539</th>\n",
       "      <td>2017-01-01 01:00:00</td>\n",
       "      <td>966.60</td>\n",
       "      <td>966.60</td>\n",
       "      <td>966.60</td>\n",
       "      <td>966.60</td>\n",
       "      <td>0.117000</td>\n",
       "      <td>113.092200</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>525479</th>\n",
       "      <td>2017-01-01 02:00:00</td>\n",
       "      <td>964.35</td>\n",
       "      <td>964.35</td>\n",
       "      <td>964.35</td>\n",
       "      <td>964.35</td>\n",
       "      <td>0.044955</td>\n",
       "      <td>43.352316</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>525419</th>\n",
       "      <td>2017-01-01 03:00:00</td>\n",
       "      <td>963.97</td>\n",
       "      <td>963.97</td>\n",
       "      <td>963.97</td>\n",
       "      <td>963.97</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>525359</th>\n",
       "      <td>2017-01-01 04:00:00</td>\n",
       "      <td>960.61</td>\n",
       "      <td>960.61</td>\n",
       "      <td>960.60</td>\n",
       "      <td>960.60</td>\n",
       "      <td>0.011157</td>\n",
       "      <td>10.717626</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>525299</th>\n",
       "      <td>2017-01-01 05:00:00</td>\n",
       "      <td>963.46</td>\n",
       "      <td>963.46</td>\n",
       "      <td>963.46</td>\n",
       "      <td>963.46</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2064801</th>\n",
       "      <td>2022-02-28 23:00:00</td>\n",
       "      <td>43085.30</td>\n",
       "      <td>43250.00</td>\n",
       "      <td>43027.93</td>\n",
       "      <td>43053.74</td>\n",
       "      <td>3.016036</td>\n",
       "      <td>129851.644413</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2064741</th>\n",
       "      <td>2022-03-01 00:00:00</td>\n",
       "      <td>43221.71</td>\n",
       "      <td>43244.68</td>\n",
       "      <td>43214.95</td>\n",
       "      <td>43237.60</td>\n",
       "      <td>0.305643</td>\n",
       "      <td>13215.276262</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2064681</th>\n",
       "      <td>2022-03-01 01:00:00</td>\n",
       "      <td>43594.14</td>\n",
       "      <td>43594.14</td>\n",
       "      <td>43530.98</td>\n",
       "      <td>43548.62</td>\n",
       "      <td>0.124288</td>\n",
       "      <td>5412.571754</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2064621</th>\n",
       "      <td>2022-03-01 02:00:00</td>\n",
       "      <td>43290.16</td>\n",
       "      <td>43301.26</td>\n",
       "      <td>43277.55</td>\n",
       "      <td>43296.76</td>\n",
       "      <td>0.243616</td>\n",
       "      <td>10547.801236</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2064561</th>\n",
       "      <td>2022-03-01 03:00:00</td>\n",
       "      <td>43160.97</td>\n",
       "      <td>43161.72</td>\n",
       "      <td>43160.97</td>\n",
       "      <td>43161.21</td>\n",
       "      <td>0.006174</td>\n",
       "      <td>266.494575</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>44584 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Day      open      high       low     Price  \\\n",
       "525539  2017-01-01 01:00:00    966.60    966.60    966.60    966.60   \n",
       "525479  2017-01-01 02:00:00    964.35    964.35    964.35    964.35   \n",
       "525419  2017-01-01 03:00:00    963.97    963.97    963.97    963.97   \n",
       "525359  2017-01-01 04:00:00    960.61    960.61    960.60    960.60   \n",
       "525299  2017-01-01 05:00:00    963.46    963.46    963.46    963.46   \n",
       "...                     ...       ...       ...       ...       ...   \n",
       "2064801 2022-02-28 23:00:00  43085.30  43250.00  43027.93  43053.74   \n",
       "2064741 2022-03-01 00:00:00  43221.71  43244.68  43214.95  43237.60   \n",
       "2064681 2022-03-01 01:00:00  43594.14  43594.14  43530.98  43548.62   \n",
       "2064621 2022-03-01 02:00:00  43290.16  43301.26  43277.55  43296.76   \n",
       "2064561 2022-03-01 03:00:00  43160.97  43161.72  43160.97  43161.21   \n",
       "\n",
       "         Volume BTC         Volume  Classification  \n",
       "525539     0.117000     113.092200             1.0  \n",
       "525479     0.044955      43.352316             1.0  \n",
       "525419     0.000000       0.000000             1.0  \n",
       "525359     0.011157      10.717626             1.0  \n",
       "525299     0.000000       0.000000             1.0  \n",
       "...             ...            ...             ...  \n",
       "2064801    3.016036  129851.644413             2.0  \n",
       "2064741    0.305643   13215.276262             2.0  \n",
       "2064681    0.124288    5412.571754             2.0  \n",
       "2064621    0.243616   10547.801236             2.0  \n",
       "2064561    0.006174     266.494575             2.0  \n",
       "\n",
       "[44584 rows x 8 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classi = Classifier(max_drop=-0.05, max_raise=0.05, max_period=240)\n",
    "historical_data = load_data()\n",
    "\n",
    "historical_data['minute'] = historical_data.Day.dt.minute\n",
    "historical_data = historical_data[historical_data.minute == 0]\n",
    "historical_data.drop('minute', axis=1, inplace=True)\n",
    "\n",
    "classified_data = classi.classify_data(historical_data)\n",
    "classified_data.to_csv('./historicals/BTC-min-005-005_with_max_period.csv')\n",
    "classified_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Day</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>Price</th>\n",
       "      <th>Volume BTC</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Classification</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-01-01 01:00:00</td>\n",
       "      <td>966.60</td>\n",
       "      <td>966.60</td>\n",
       "      <td>966.60</td>\n",
       "      <td>966.60</td>\n",
       "      <td>0.117000</td>\n",
       "      <td>113.092200</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-01-01 02:00:00</td>\n",
       "      <td>964.35</td>\n",
       "      <td>964.35</td>\n",
       "      <td>964.35</td>\n",
       "      <td>964.35</td>\n",
       "      <td>0.044955</td>\n",
       "      <td>43.352316</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-01-01 03:00:00</td>\n",
       "      <td>963.97</td>\n",
       "      <td>963.97</td>\n",
       "      <td>963.97</td>\n",
       "      <td>963.97</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017-01-01 04:00:00</td>\n",
       "      <td>960.61</td>\n",
       "      <td>960.61</td>\n",
       "      <td>960.60</td>\n",
       "      <td>960.60</td>\n",
       "      <td>0.011157</td>\n",
       "      <td>10.717626</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017-01-01 05:00:00</td>\n",
       "      <td>963.46</td>\n",
       "      <td>963.46</td>\n",
       "      <td>963.46</td>\n",
       "      <td>963.46</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44579</th>\n",
       "      <td>2022-02-28 23:00:00</td>\n",
       "      <td>43085.30</td>\n",
       "      <td>43250.00</td>\n",
       "      <td>43027.93</td>\n",
       "      <td>43053.74</td>\n",
       "      <td>3.016036</td>\n",
       "      <td>129851.644413</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44580</th>\n",
       "      <td>2022-03-01 00:00:00</td>\n",
       "      <td>43221.71</td>\n",
       "      <td>43244.68</td>\n",
       "      <td>43214.95</td>\n",
       "      <td>43237.60</td>\n",
       "      <td>0.305643</td>\n",
       "      <td>13215.276262</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44581</th>\n",
       "      <td>2022-03-01 01:00:00</td>\n",
       "      <td>43594.14</td>\n",
       "      <td>43594.14</td>\n",
       "      <td>43530.98</td>\n",
       "      <td>43548.62</td>\n",
       "      <td>0.124288</td>\n",
       "      <td>5412.571754</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44582</th>\n",
       "      <td>2022-03-01 02:00:00</td>\n",
       "      <td>43290.16</td>\n",
       "      <td>43301.26</td>\n",
       "      <td>43277.55</td>\n",
       "      <td>43296.76</td>\n",
       "      <td>0.243616</td>\n",
       "      <td>10547.801236</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44583</th>\n",
       "      <td>2022-03-01 03:00:00</td>\n",
       "      <td>43160.97</td>\n",
       "      <td>43161.72</td>\n",
       "      <td>43160.97</td>\n",
       "      <td>43161.21</td>\n",
       "      <td>0.006174</td>\n",
       "      <td>266.494575</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>44584 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Day      open      high       low     Price  Volume BTC  \\\n",
       "0     2017-01-01 01:00:00    966.60    966.60    966.60    966.60    0.117000   \n",
       "1     2017-01-01 02:00:00    964.35    964.35    964.35    964.35    0.044955   \n",
       "2     2017-01-01 03:00:00    963.97    963.97    963.97    963.97    0.000000   \n",
       "3     2017-01-01 04:00:00    960.61    960.61    960.60    960.60    0.011157   \n",
       "4     2017-01-01 05:00:00    963.46    963.46    963.46    963.46    0.000000   \n",
       "...                   ...       ...       ...       ...       ...         ...   \n",
       "44579 2022-02-28 23:00:00  43085.30  43250.00  43027.93  43053.74    3.016036   \n",
       "44580 2022-03-01 00:00:00  43221.71  43244.68  43214.95  43237.60    0.305643   \n",
       "44581 2022-03-01 01:00:00  43594.14  43594.14  43530.98  43548.62    0.124288   \n",
       "44582 2022-03-01 02:00:00  43290.16  43301.26  43277.55  43296.76    0.243616   \n",
       "44583 2022-03-01 03:00:00  43160.97  43161.72  43160.97  43161.21    0.006174   \n",
       "\n",
       "              Volume  Classification  \n",
       "0         113.092200             1.0  \n",
       "1          43.352316             1.0  \n",
       "2           0.000000             1.0  \n",
       "3          10.717626             1.0  \n",
       "4           0.000000             1.0  \n",
       "...              ...             ...  \n",
       "44579  129851.644413             2.0  \n",
       "44580   13215.276262             2.0  \n",
       "44581    5412.571754             2.0  \n",
       "44582   10547.801236             2.0  \n",
       "44583     266.494575             2.0  \n",
       "\n",
       "[44584 rows x 8 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classified_data_path = './historicals/BTC-min-005-005_with_max_period.csv'\n",
    "data = pd.read_csv(classified_data_path, parse_dates=['Day']).iloc[:, 1:]\n",
    "data.sort_values(\"Day\", inplace=True)\n",
    "data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQ_LEN = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = preprocess(sequence_length=SEQ_LEN)\n",
    "training, validation, testing  = processor.preprocess(dataframe=data, validation_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "class RNN_module(nn.Module):\n",
    "    def __init__(self, hidden_size, input_size, output_size, num_layers):\n",
    "        super(RNN_module, self).__init__()\n",
    "        self._num_layers = num_layers\n",
    "        self._input_size = input_size\n",
    "        self._hidden_size = hidden_size\n",
    "        self._output_size = output_size\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size = self._input_size, hidden_size = self._hidden_size, \n",
    "                            num_layers = self._num_layers, batch_first = True)\n",
    "        self.fc = nn.Linear(in_features=self._hidden_size, out_features= self._output_size)\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"RNN LSTM Model w/ {self._input_size} features and {self._num_layers} layers and {self._hidden_size} of hidden size\"\n",
    "\n",
    "    def forward(self, input):\n",
    "        h_0 = torch.zeros(self._num_layers, input.size(0), self._hidden_size, dtype=torch.float64)\n",
    "        c_0 = torch.zeros(self._num_layers, input.size(0), self._hidden_size, dtype=torch.float64)\n",
    "\n",
    "        lstm_output, (h_n, c_n) = self.lstm(input)\n",
    "\n",
    "        pred = self.fc(lstm_output[:, -1, :])\n",
    "        \n",
    "        return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "learning_rate = 0.01\n",
    "dim_size = training._features.size(dim=-1)\n",
    "hidden_size = 60\n",
    "batch_size = 64\n",
    "number_of_classes = training[1][1].shape[0]\n",
    "\n",
    "train_dataloader = DataLoader(training, batch_size = batch_size, shuffle = True)\n",
    "validation_dataloader = DataLoader(validation, batch_size = batch_size, shuffle = True)\n",
    "test_dataloader = DataLoader(testing, batch_size = batch_size, shuffle = False)\n",
    "\n",
    "model = RNN_module(hidden_size = hidden_size, input_size = dim_size,\n",
    "                     output_size = number_of_classes, num_layers = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.5919, 0.5094, 0.8987])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = training._labels\n",
    "classes = torch.unique(labels, dim = 0)\n",
    "\n",
    "freq = {}\n",
    "for _class in classes:\n",
    "    freq[_class.item()] = 0\n",
    "\n",
    "weights = torch.zeros((classes.shape[0],))\n",
    "total = torch.tensor(len(training))\n",
    "\n",
    "for input in range(training._beg, training._end):\n",
    "    _, output = torch.max(training[input][1], 0)\n",
    "    freq[output.item()] += 1\n",
    "\n",
    "\n",
    "for idx, _class in enumerate(classes):\n",
    "    weights[idx] +=  1 - freq[_class.item()]/total\n",
    "\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss(weight= weights)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECKPOINT_PATH = './models_parameters/LSTM/checkpoints_2/'\n",
    "BEST_PATH = './models_parameters/LSTM/best_model.pth'\n",
    "\n",
    "def epoch_training(train_dataloader, epoch, total_epochs):\n",
    "    running_correct = 0\n",
    "    running_loss = 0.0\n",
    "    n_of_steps = len(train_dataloader)\n",
    "\n",
    "    for current_batch, (sequence, label) in enumerate(train_dataloader):\n",
    "        #forward: we are calculating the loss given the parameters\n",
    "        outputs = model(sequence)\n",
    "        loss = criterion(input=outputs, target = label.float())\n",
    "\n",
    "        #backward: lets update the parameters given the current loss\n",
    "        optimizer.zero_grad() #nullifies the current gradients. If you don't do this, gradients will be added up (you don't want that)\n",
    "        loss.backward() #computates the bwrd-prop gradient for each model parameter\n",
    "        optimizer.step() #updates the model current parameter using the gradients.\n",
    "\n",
    "        _, predictions = torch.max(outputs, 1)\n",
    "        _, correct = torch.max(label, 1)\n",
    "\n",
    "        running_correct += (predictions == correct).sum().item()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        if (current_batch + 1) % 50 == 0:\n",
    "            print(f\"epoch {epoch+1}/{total_epochs}, current step(batch): {current_batch+1}/{n_of_steps}, loss = {loss.item():.4f} \")\n",
    "            writer.add_scalar('training loss: ', running_loss/50, epoch * n_of_steps + current_batch)\n",
    "            writer.add_scalar('accuracy: ', running_correct/50, epoch * n_of_steps + current_batch)\n",
    "            running_loss = 0.0\n",
    "            running_correct = 0\n",
    "\n",
    "    writer.add_scalar('Epoch loss: ', loss, epoch + 1)\n",
    "\n",
    "\n",
    "def epoch_validate(validation_dataloader, epoch, total_epochs):\n",
    "    with torch.no_grad():\n",
    "        n_corrects = 0\n",
    "        n_samples = 0\n",
    "        \n",
    "        for current_batch, (sequence, label) in enumerate(validation_dataloader):\n",
    "\n",
    "            #forward: we are calculating the loss given the parameters\n",
    "            outputs = model(sequence)\n",
    "            _, predictions = torch.max(outputs, 1)\n",
    "            \n",
    "            n_samples += outputs.shape[0]\n",
    "            n_corrects += (predictions == label).sum().item()\n",
    "\n",
    "        acc = 100.0 * n_corrects / n_samples\n",
    "\n",
    "        print(f\"epoch {epoch+1}/{total_epochs} accuracy: {acc}\")\n",
    "        writer.add_scalar('Validation Accuracy: ', acc, epoch+1)\n",
    "\n",
    "    return acc\n",
    "\n",
    "\n",
    "def train_loop(train_dataloader: DataLoader, validation_dataloader: DataLoader, epochs: int):\n",
    "    \n",
    "    max_accuracy = 0\n",
    "    is_best = False\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        epoch_training(train_dataloader, epoch, epochs)\n",
    "\n",
    "        accuracy = epoch_validate(validation_dataloader, epoch, epochs)\n",
    "\n",
    "        if accuracy > max_accuracy:\n",
    "            is_best = True\n",
    "        \n",
    "        checkpoint = {\n",
    "            'epoch': epoch,\n",
    "            'model_state': model.state_dict(),\n",
    "            'optim_state': optimizer.state_dict()\n",
    "        }\n",
    "\n",
    "        if is_best:\n",
    "            torch.save(checkpoint, BEST_PATH)\n",
    "        \n",
    "        torch.save(checkpoint, CHECKPOINT_PATH+f'model_{epoch+1}.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_loop(test_dataloader: DataLoader, model: nn.Module):\n",
    "    with torch.no_grad():\n",
    "        n_corrects = 0\n",
    "        n_samples = 0\n",
    "\n",
    "        for current_batch, (sequence, label) in enumerate(test_dataloader):\n",
    "            #forward: we are calculating the loss given the parameters\n",
    "            outputs = model(sequence)\n",
    "            _, predictions = torch.max(outputs, 1)\n",
    "\n",
    "            n_samples += outputs.shape[0]\n",
    "            n_corrects += (predictions == label).sum().item()\n",
    "\n",
    "            if (current_batch + 1) % 200 == 0:\n",
    "                print(f\"test batch: {current_batch+1}/{len(test_dataloader)}, current accuracy: {100 * n_corrects / n_samples}\")\n",
    "\n",
    "        acc = 100.0 * n_corrects / n_samples\n",
    "        print(f\"final test accuracy: {acc}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1/10, current step(batch): 50/485, loss = 0.5750 \n",
      "epoch 1/10, current step(batch): 100/485, loss = 0.5629 \n",
      "epoch 1/10, current step(batch): 150/485, loss = 0.6872 \n",
      "epoch 1/10, current step(batch): 200/485, loss = 0.5663 \n",
      "epoch 1/10, current step(batch): 250/485, loss = 0.6691 \n",
      "epoch 1/10, current step(batch): 300/485, loss = 0.6214 \n",
      "epoch 1/10, current step(batch): 350/485, loss = 0.5776 \n",
      "epoch 1/10, current step(batch): 400/485, loss = 0.5612 \n",
      "epoch 1/10, current step(batch): 450/485, loss = 0.6478 \n",
      "epoch 1/10 accuracy: 56.43631436314363\n",
      "epoch 2/10, current step(batch): 50/485, loss = 0.5914 \n",
      "epoch 2/10, current step(batch): 100/485, loss = 0.6546 \n",
      "epoch 2/10, current step(batch): 150/485, loss = 0.5997 \n",
      "epoch 2/10, current step(batch): 200/485, loss = 0.5706 \n",
      "epoch 2/10, current step(batch): 250/485, loss = 0.6028 \n",
      "epoch 2/10, current step(batch): 300/485, loss = 0.5698 \n",
      "epoch 2/10, current step(batch): 350/485, loss = 0.5438 \n",
      "epoch 2/10, current step(batch): 400/485, loss = 0.6319 \n",
      "epoch 2/10, current step(batch): 450/485, loss = 0.6955 \n",
      "epoch 2/10 accuracy: 37.99683830171635\n",
      "epoch 3/10, current step(batch): 50/485, loss = 0.5863 \n",
      "epoch 3/10, current step(batch): 100/485, loss = 0.5679 \n",
      "epoch 3/10, current step(batch): 150/485, loss = 0.5836 \n",
      "epoch 3/10, current step(batch): 200/485, loss = 0.6841 \n",
      "epoch 3/10, current step(batch): 250/485, loss = 0.5951 \n",
      "epoch 3/10, current step(batch): 300/485, loss = 0.6788 \n",
      "epoch 3/10, current step(batch): 350/485, loss = 0.5851 \n",
      "epoch 3/10, current step(batch): 400/485, loss = 0.5464 \n",
      "epoch 3/10, current step(batch): 450/485, loss = 0.6074 \n",
      "epoch 3/10 accuracy: 37.99683830171635\n",
      "epoch 4/10, current step(batch): 50/485, loss = 0.5668 \n",
      "epoch 4/10, current step(batch): 100/485, loss = 0.6064 \n",
      "epoch 4/10, current step(batch): 150/485, loss = 0.5822 \n",
      "epoch 4/10, current step(batch): 200/485, loss = 0.5172 \n",
      "epoch 4/10, current step(batch): 250/485, loss = 0.6167 \n",
      "epoch 4/10, current step(batch): 300/485, loss = 0.5832 \n",
      "epoch 4/10, current step(batch): 350/485, loss = 0.5933 \n",
      "epoch 4/10, current step(batch): 400/485, loss = 0.5990 \n",
      "epoch 4/10, current step(batch): 450/485, loss = 0.5654 \n",
      "epoch 4/10 accuracy: 37.99683830171635\n",
      "epoch 5/10, current step(batch): 50/485, loss = 0.6250 \n",
      "epoch 5/10, current step(batch): 100/485, loss = 0.5065 \n",
      "epoch 5/10, current step(batch): 150/485, loss = 0.5434 \n",
      "epoch 5/10, current step(batch): 200/485, loss = 0.6616 \n",
      "epoch 5/10, current step(batch): 250/485, loss = 0.5744 \n",
      "epoch 5/10, current step(batch): 300/485, loss = 0.5793 \n",
      "epoch 5/10, current step(batch): 350/485, loss = 0.5698 \n",
      "epoch 5/10, current step(batch): 400/485, loss = 0.6478 \n",
      "epoch 5/10, current step(batch): 450/485, loss = 0.7219 \n",
      "epoch 5/10 accuracy: 56.43631436314363\n",
      "epoch 6/10, current step(batch): 50/485, loss = 0.5800 \n",
      "epoch 6/10, current step(batch): 100/485, loss = 0.6256 \n",
      "epoch 6/10, current step(batch): 150/485, loss = 0.6332 \n",
      "epoch 6/10, current step(batch): 200/485, loss = 0.7433 \n",
      "epoch 6/10, current step(batch): 250/485, loss = 0.5429 \n",
      "epoch 6/10, current step(batch): 300/485, loss = 0.5705 \n",
      "epoch 6/10, current step(batch): 350/485, loss = 0.5572 \n",
      "epoch 6/10, current step(batch): 400/485, loss = 0.6105 \n",
      "epoch 6/10, current step(batch): 450/485, loss = 0.6003 \n",
      "epoch 6/10 accuracy: 56.43631436314363\n",
      "epoch 7/10, current step(batch): 50/485, loss = 0.5625 \n",
      "epoch 7/10, current step(batch): 100/485, loss = 0.5634 \n",
      "epoch 7/10, current step(batch): 150/485, loss = 0.5857 \n",
      "epoch 7/10, current step(batch): 200/485, loss = 0.6275 \n",
      "epoch 7/10, current step(batch): 250/485, loss = 0.6060 \n",
      "epoch 7/10, current step(batch): 300/485, loss = 0.6186 \n",
      "epoch 7/10, current step(batch): 350/485, loss = 0.5915 \n",
      "epoch 7/10, current step(batch): 400/485, loss = 0.5472 \n",
      "epoch 7/10, current step(batch): 450/485, loss = 0.5652 \n",
      "epoch 7/10 accuracy: 56.43631436314363\n",
      "epoch 8/10, current step(batch): 50/485, loss = 0.6507 \n",
      "epoch 8/10, current step(batch): 100/485, loss = 0.5517 \n",
      "epoch 8/10, current step(batch): 150/485, loss = 0.6524 \n",
      "epoch 8/10, current step(batch): 200/485, loss = 0.5635 \n",
      "epoch 8/10, current step(batch): 250/485, loss = 0.6205 \n",
      "epoch 8/10, current step(batch): 300/485, loss = 0.6022 \n",
      "epoch 8/10, current step(batch): 350/485, loss = 0.5877 \n",
      "epoch 8/10, current step(batch): 400/485, loss = 0.5688 \n",
      "epoch 8/10, current step(batch): 450/485, loss = 0.7169 \n",
      "epoch 8/10 accuracy: 37.99683830171635\n",
      "epoch 9/10, current step(batch): 50/485, loss = 0.5963 \n",
      "epoch 9/10, current step(batch): 100/485, loss = 0.6717 \n",
      "epoch 9/10, current step(batch): 150/485, loss = 0.6511 \n",
      "epoch 9/10, current step(batch): 200/485, loss = 0.5642 \n",
      "epoch 9/10, current step(batch): 250/485, loss = 0.6099 \n",
      "epoch 9/10, current step(batch): 300/485, loss = 0.5916 \n",
      "epoch 9/10, current step(batch): 350/485, loss = 0.6065 \n",
      "epoch 9/10, current step(batch): 400/485, loss = 0.6630 \n",
      "epoch 9/10, current step(batch): 450/485, loss = 0.5666 \n",
      "epoch 9/10 accuracy: 56.43631436314363\n",
      "epoch 10/10, current step(batch): 50/485, loss = 0.6826 \n",
      "epoch 10/10, current step(batch): 100/485, loss = 0.6255 \n",
      "epoch 10/10, current step(batch): 150/485, loss = 0.6389 \n",
      "epoch 10/10, current step(batch): 200/485, loss = 0.5504 \n",
      "epoch 10/10, current step(batch): 250/485, loss = 0.5849 \n",
      "epoch 10/10, current step(batch): 300/485, loss = 0.5887 \n",
      "epoch 10/10, current step(batch): 350/485, loss = 0.5154 \n",
      "epoch 10/10, current step(batch): 400/485, loss = 0.5457 \n",
      "epoch 10/10, current step(batch): 450/485, loss = 0.5575 \n",
      "epoch 10/10 accuracy: 56.43631436314363\n",
      "final test accuracy: 48.34048317904719\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "train_loop(train_dataloader=train_dataloader, validation_dataloader = validation_dataloader, epochs=epochs)\n",
    "test_loop(test_dataloader=test_dataloader) #curioso: quando o dataloader tava em lista (tuple) o modelo tava treinando bem mais rápido..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final test accuracy: 48.34048317904719\n"
     ]
    }
   ],
   "source": [
    "test_loop(test_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using best model in validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN LSTM Model w/ 6 features and 1 layers and 60 of hidden size\n"
     ]
    }
   ],
   "source": [
    "best_model = RNN_module(hidden_size = hidden_size, input_size = dim_size,\n",
    "                     output_size = number_of_classes, num_layers = 1)\n",
    "\n",
    "print(f'{best_model}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model found at 9ºepoch\n",
      "final test accuracy: 48.34048317904719\n"
     ]
    }
   ],
   "source": [
    "best_model = RNN_module(hidden_size = hidden_size, input_size = dim_size,\n",
    "                     output_size = number_of_classes, num_layers = 1)\n",
    "checkpoint = torch.load(BEST_PATH)\n",
    "print(f'Model type: {best_model}')\n",
    "print(f'Best performing model found at {checkpoint[\"epoch\"]}ºepoch')\n",
    "\n",
    "best_model.load_state_dict(state_dict=checkpoint['model_state'], strict=True)\n",
    "best_model.eval()\n",
    "\n",
    "test_loop(test_dataloader=test_dataloader, model=best_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit ('Quant-Trader')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fc0d1250504fb5812c35f9d788d06f80c95c7322289624d7d7f522231bf576a4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
