{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from utils.nn_data_classifier import load_data, Classifier\n",
    "from utils.preprocess import preprocess\n",
    "from utils.nn_data_classifier import Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classi = Classifier(max_drop=-0.10, max_raise=0.10)\n",
    "# historical_data = load_data()\n",
    "\n",
    "# classified_data = classi.classify_data(historical_data)\n",
    "# classified_data.to_csv('./historicals/BTC-min-01-01.csv')\n",
    "# classified_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Day</th>\n",
       "      <th>Price</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Classification</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-01-01 00:01:00</td>\n",
       "      <td>966.34</td>\n",
       "      <td>7.610000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-01-01 00:02:00</td>\n",
       "      <td>966.37</td>\n",
       "      <td>8.087376</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-01-01 00:03:00</td>\n",
       "      <td>966.37</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017-01-01 00:04:00</td>\n",
       "      <td>966.37</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017-01-01 00:05:00</td>\n",
       "      <td>966.43</td>\n",
       "      <td>0.107000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2674550</th>\n",
       "      <td>2022-02-28 15:12:00</td>\n",
       "      <td>39764.94</td>\n",
       "      <td>1.284295</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2674551</th>\n",
       "      <td>2022-02-28 15:13:00</td>\n",
       "      <td>39774.64</td>\n",
       "      <td>9.949596</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2674552</th>\n",
       "      <td>2022-02-28 15:14:00</td>\n",
       "      <td>39797.66</td>\n",
       "      <td>0.661230</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2674553</th>\n",
       "      <td>2022-02-28 15:15:00</td>\n",
       "      <td>39848.52</td>\n",
       "      <td>2.110273</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2674554</th>\n",
       "      <td>2022-02-28 15:16:00</td>\n",
       "      <td>39959.09</td>\n",
       "      <td>20.791850</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2674554 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Day     Price     Volume  Classification\n",
       "0       2017-01-01 00:01:00    966.34   7.610000             1.0\n",
       "1       2017-01-01 00:02:00    966.37   8.087376             1.0\n",
       "2       2017-01-01 00:03:00    966.37   0.000000             1.0\n",
       "3       2017-01-01 00:04:00    966.37   0.000000             1.0\n",
       "4       2017-01-01 00:05:00    966.43   0.107000             1.0\n",
       "...                     ...       ...        ...             ...\n",
       "2674550 2022-02-28 15:12:00  39764.94   1.284295             1.0\n",
       "2674551 2022-02-28 15:13:00  39774.64   9.949596             1.0\n",
       "2674552 2022-02-28 15:14:00  39797.66   0.661230             1.0\n",
       "2674553 2022-02-28 15:15:00  39848.52   2.110273             1.0\n",
       "2674554 2022-02-28 15:16:00  39959.09  20.791850             1.0\n",
       "\n",
       "[2674554 rows x 4 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classified_data_path = './historicals/BTC-min-01-01.csv'\n",
    "data = pd.read_csv(classified_data_path, parse_dates=['Day']).iloc[:, 1:]\n",
    "data.sort_values(\"Day\", inplace=True)\n",
    "data = data[data.Classification != 2]\n",
    "data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQ_LEN = 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ayrton/Projects/Quant-Trader/utils/preprocess.py:161: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return np.array(sequential_data)\n"
     ]
    }
   ],
   "source": [
    "processor = preprocess(sequence_length=SEQ_LEN)\n",
    "training, testing  = processor.preprocess(dataframe=data, validation_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch.nn.functional import relu\n",
    "class RNN_module(nn.Module):\n",
    "    def __init__(self, hidden_size, input_size, output_size, num_layers):\n",
    "        super().__init__()\n",
    "        self._num_layers = num_layers\n",
    "        self._input_size = input_size\n",
    "        self._hidden_size = hidden_size\n",
    "        self._output_size = output_size\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size = self._input_size, hidden_size = self._hidden_size, \n",
    "                            num_layers = self._num_layers, batch_first = True)\n",
    "\n",
    "        self.fc = nn.Linear(in_features=self._hidden_size, out_features= self._output_size)\n",
    "\n",
    "    def forward(self, input):\n",
    "        h_0 = torch.zeros(self._num_layers, input.size(0), self._hidden_size).double()\n",
    "        c_0 = torch.zeros(self._num_layers, input.size(0), self._hidden_size).double()\n",
    "\n",
    "        input = input.double()\n",
    "        print(h_0.dtype)\n",
    "\n",
    "        lstm_output, h_n, c_n = self.lstm(input, (h_0, c_0))\n",
    "\n",
    "        logist = self.fc(lstm_output[:, -1, :])\n",
    "        scores = relu(logist)\n",
    "        return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "learning_rate = 0.01\n",
    "dim_size = training._sequence.size(dim=-1)\n",
    "hidden_size = 60\n",
    "batch_size = 64\n",
    "number_of_classes = training._labels.size(dim=-1)\n",
    "train_dataloader = DataLoader(training, batch_size = batch_size, shuffle = True)\n",
    "test_dataloader = DataLoader(testing, batch_size = batch_size, shuffle = True)\n",
    "\n",
    "model = RNN_module(hidden_size = hidden_size, input_size = dim_size,\n",
    "                     output_size = number_of_classes, num_layers = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.5736, 0.4264], dtype=torch.float64)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = training._labels\n",
    "classes = torch.unique(labels, dim = 0)\n",
    "weights = torch.zeros((classes.shape[0],))\n",
    "total = torch.tensor(len(training))\n",
    "\n",
    "for idx, _class in enumerate(classes):\n",
    "    weights = weights + _class * (1 - labels[labels == _class].sum() / total)\n",
    "\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss(weight= weights)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(train_dataloader: DataLoader, epochs: int):\n",
    "    n_of_steps = len(train_dataloader) #total number of batches\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        for current_batch, (sequence, label) in enumerate(train_dataloader):\n",
    "\n",
    "            #forward: we are calculating the loss given the parameters\n",
    "            outputs = model(sequence)\n",
    "            loss = criterion(input=outputs, target = label)\n",
    "\n",
    "            #backward: lets update the parameters given the current loss\n",
    "            optimizer.zero_grad() #nullifies the current gradients. If you don't do this, gradients will be added up (you don't want that)\n",
    "            loss.backward() #computates the bwrd-prop gradient for each model parameter\n",
    "            optimizer.step() #updates the model current parameter using the gradients.\n",
    "\n",
    "            if (current_batch + 1) % 500 == 0:\n",
    "                print(f\"epoch {epoch+1}/{epochs}, current step(batch): {current_batch+1}/{n_of_steps}, loss = {loss.item():.4f} \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_loop(test_dataloader: DataLoader):\n",
    "    with torch.no_grad():\n",
    "        n_corrects = 0\n",
    "        n_samples = 0\n",
    "\n",
    "        for current_batch, (sequence, label) in enumerate(train_dataloader):\n",
    "\n",
    "            #forward: we are calculating the loss given the parameters\n",
    "            outputs = model(sequence)\n",
    "            predictions = (outputs == outputs.max(dim=1, keepdim=True)[0]).to(dtype=torch.int32) #one hot encode the output through max-threshold\n",
    "            \n",
    "            n_samples += outputs.shape[0]\n",
    "            n_correct += (predictions == label).sum().item() / predictions.shape[1]\n",
    "\n",
    "            acc = 100.0 * n_corrects / n_samples\n",
    "            print(f\"accuracy: {acc}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float64\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "expected scalar type Double but found Float",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [55], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m epochs \u001b[39m=\u001b[39m \u001b[39m10\u001b[39m\n\u001b[0;32m----> 2\u001b[0m train_loop(train_dataloader\u001b[39m=\u001b[39;49mtrain_dataloader, epochs\u001b[39m=\u001b[39;49mepochs)\n\u001b[1;32m      3\u001b[0m test_loop(test_dataloader\u001b[39m=\u001b[39mtest_dataloader)\n",
      "Cell \u001b[0;32mIn [38], line 8\u001b[0m, in \u001b[0;36mtrain_loop\u001b[0;34m(train_dataloader, epochs)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(epochs):\n\u001b[1;32m      5\u001b[0m     \u001b[39mfor\u001b[39;00m current_batch, (sequence, label) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(train_dataloader):\n\u001b[1;32m      6\u001b[0m \n\u001b[1;32m      7\u001b[0m         \u001b[39m#forward: we are calculating the loss given the parameters\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m         outputs \u001b[39m=\u001b[39m model(sequence)\n\u001b[1;32m      9\u001b[0m         loss \u001b[39m=\u001b[39m criterion(\u001b[39minput\u001b[39m\u001b[39m=\u001b[39moutputs, target \u001b[39m=\u001b[39m label)\n\u001b[1;32m     11\u001b[0m         \u001b[39m#backward: lets update the parameters given the current loss\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.4/envs/Quant-Trader/lib/python3.10/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn [53], line 23\u001b[0m, in \u001b[0;36mRNN_module.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m \u001b[39minput\u001b[39m\u001b[39m.\u001b[39mdouble()\n\u001b[1;32m     21\u001b[0m \u001b[39mprint\u001b[39m(h_0\u001b[39m.\u001b[39mdtype)\n\u001b[0;32m---> 23\u001b[0m lstm_output, h_n, c_n \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlstm(\u001b[39minput\u001b[39;49m, (h_0, c_0))\n\u001b[1;32m     25\u001b[0m logist \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc(lstm_output[:, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, :])\n\u001b[1;32m     26\u001b[0m scores \u001b[39m=\u001b[39m relu(logist)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.4/envs/Quant-Trader/lib/python3.10/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.4/envs/Quant-Trader/lib/python3.10/site-packages/torch/nn/modules/rnn.py:774\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    772\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcheck_forward_args(\u001b[39minput\u001b[39m, hx, batch_sizes)\n\u001b[1;32m    773\u001b[0m \u001b[39mif\u001b[39;00m batch_sizes \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 774\u001b[0m     result \u001b[39m=\u001b[39m _VF\u001b[39m.\u001b[39;49mlstm(\u001b[39minput\u001b[39;49m, hx, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_flat_weights, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnum_layers,\n\u001b[1;32m    775\u001b[0m                       \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdropout, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbidirectional, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbatch_first)\n\u001b[1;32m    776\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    777\u001b[0m     result \u001b[39m=\u001b[39m _VF\u001b[39m.\u001b[39mlstm(\u001b[39minput\u001b[39m, batch_sizes, hx, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_flat_weights, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias,\n\u001b[1;32m    778\u001b[0m                       \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_layers, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbidirectional)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: expected scalar type Double but found Float"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "train_loop(train_dataloader=train_dataloader, epochs=epochs)\n",
    "test_loop(test_dataloader=test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit ('Quant-Trader')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fc0d1250504fb5812c35f9d788d06f80c95c7322289624d7d7f522231bf576a4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
